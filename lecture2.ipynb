			<span style="color:#280099"> __Machine learning__ </span>  <span style="color:#280099"> __ __ </span>  <span style="color:#280099"> __Lecture 2__ </span>

<span style="color:#000000">Marcin Wolter</span>

<span style="color:#000000"> _IFJ PAN_ </span>

<span style="color:#000000"> _21 October 2020_ </span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_20.png)

<span style="color:#00000A">Decorrelation and Principal Component Analysis \(PCA\) </span>

<span style="color:#00000A">Overtraining</span>

<span style="color:#00000A">Simple non\-linear methods like k\-nearest neighbors and decision trees\.</span>

<span style="color:#C9211E"> __All slides will be here:    __ </span>  <span style="color:#C9211E"> __[https://indico\.ifj\.edu\.pl/event/397/](https://indico.ifj.edu.pl/event/397/)__ </span>

<span style="color:#800000"> __Glen Cowan book and slides on statistics__ </span>

* <span style="color:#000000">Fantastic Glen Cowan’s courses on statistics can be found here:</span>
  * <span style="color:#000000">[http://www\.pp\.rhul\.ac\.uk/~cowan/stat\_cern\.html](http://www.pp.rhul.ac.uk/~cowan/stat_cern.html)</span>
  * <span style="color:#000000">[https://www\.pp\.rhul\.ac\.uk/~cowan/stat\_course\.html](https://www.pp.rhul.ac.uk/~cowan/stat_course.html)</span>
* <span style="color:#000000">and his book can be found here:</span>
  * <span style="color:#000000">[http://www\.sherrytowers\.com/cowan\_statistical\_data\_analysis\.pdf](http://www.sherrytowers.com/cowan_statistical_data_analysis.pdf)</span>

<span style="color:#800000"> __Online python course__ </span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_21.png)

If you want to take one of the online python programming or machine learning courses on the [https://www\.datacamp\.com/](https://www.datacamp.com/) platform Leszek Grzanka [leszek\.grzanka@ifj\.edu\.pl](mailto:leszek.grzanka@ifj.edu.pl) can give you a free 6 months access to this e\-learning platform\.

Just write him an e\-mail\!

<span style="color:#800000"> __Naive Bayes classifier __ </span>  <span style="color:#800000"> _\(repetition from the previous lecture\)_ </span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_22.png)

<span style="color:#FF0000">Classifier output</span>

<span style="color:#000000">Frequently called  </span>  <span style="color:#000000"> __“projected likelihood'' __ </span>  <span style="color:#000000">by physicists</span>

<span style="color:#000000">Based on the assumption\, that variables are independent \(so „naive''\): </span>

<span style="color:#000000">Output probability is </span>  <span style="color:#000000"> __a product of probabilities for all variables\.__ </span>

<span style="color:#000000">Fast and stable\, not optimal\, but in many cases sufficient\.</span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_23.png)

“Naive” assumption:

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_24.png)

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_25.png)

<span style="color:#800000"> __Removes correlation between variables by a rotation in the space of variables__ </span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_26.png)

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_27.png)

<span style="color:#800000"> __Eigenvalues and eigenvectors__ </span>

* <span style="color:#000000"> __In essence\, an eigenvector v of a linear transformation A is a non\-zero vector that\, when A is applied to it\, does not change direction\. Applying A to the eigenvector only scales the eigenvector by the scalar value λ\, called an eigenvalue\.__ </span>  <span style="color:#000000"> This condition can be written as the equation</span>
    * <span style="color:#000000">                   </span>  <span style="color:#000000"> __A__ </span>  <span style="color:#000000">\(</span>  <span style="color:#000000"> __v__ </span>  <span style="color:#000000">\) = </span>  <span style="color:#000000">λ</span>  <span style="color:#000000"> </span>  <span style="color:#000000"> __v__ </span>
* <span style="color:#000000">referred to as the eigenvalue equation or eigenequation\. In general\, λ may be any scalar\. For example\, λ may be negative\, in which case the eigenvector reverses direction as part of the scaling\, or it may be zero or even complex\.</span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_28.png)

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_29.png)

Matrix A acts by stretching the vector x\, not changing its direction\, so x is an eigenvector of A\.

Eigendecomposition of matrix

<span style="color:#800000"> __Principal Component Analysis \- PCA__ </span>

<span style="color:#000000">Task: reduce the number of dimensions minimizing the loss of information</span>

<span style="color:#000000">Finds the orthogonal base of the covariance matrix\, the eigenvectors with the smallest eigenvalues might be skipped </span>

<span style="color:#008000"> __Procedure:__ </span>

<span style="color:#000000">Find the covariance matrix </span>  <span style="color:#3333FF">Cov\(X\)</span>  <span style="color:#3333FF"> </span>

<span style="color:#000000">Find eigenvalues</span>  <span style="color:#0033CC"> </span>  <span style="color:#0033CC"></span>  <span style="color:#0033CC">i</span>  <span style="color:#000000"> and eigenvectors </span>  <span style="color:#0033CC">v</span>  <span style="color:#0033CC">i</span>

<span style="color:#000000">Skip </span>  <span style="color:#000000">smallest</span>  <span style="color:#000000"> </span>  <span style="color:#0033CC"></span>  <span style="color:#0033CC">i </span>

<span style="color:#280099">Unsupervised learning & dimensionality reduction</span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_210.gif)

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_211.png)

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_212.png)

Separable by a simple cut\,

but in this case PCA doesn’t help\.\.\.

<span style="color:#800000"> __Principal Component Analysis \(PCA\)__ </span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_213.png)

<span style="color:#000000">Nicely explained in:</span>

<span style="color:#000000">[http://docshare04\.docshare\.tips/files/12598/125983744\.pdf](http://docshare04.docshare.tips/files/12598/125983744.pdf)</span>

<span style="color:#800000"> __Correlation or covariance matrix?__ </span>

<span style="color:#000000">Mean\-centering is unnecessary if performing a principal components analysis on a correlation matrix\, as the data are already centered after calculating correlations\.</span>

<span style="color:#000000">We tend to use the covariance matrix when the variable scales are similar and the correlation matrix when variables are on different scales\.</span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_214.png)

<span style="color:#800000"> __Covariance Matrix__ </span>

<span style="color:#000000">Let X be a p\-variate random vector\. The covariance matrix of X is defined as:</span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_215.png)

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_216.png)

<span style="color:#800000"> __Correlation matrix__ </span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_217.png)

<span style="color:#800000"> __What happens if we perform PCA without normalization? Why do we normalize data?__ </span>

<span style="color:#000000">In PCA we are interested in the components that maximize the variance\. If one component \(e\.g\. human height\) varies less than another \(e\.g\. weight\) because of their respective scales \(meters vs\. kilos\)\, PCA might determine that the direction of maximal variance more closely corresponds with the ‘weight’ axis\, if those features are not scaled\. As a change in height of one meter can be considered much more important than the change in weight of one kilogram\, this is clearly incorrect\.</span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_218.png)

<span style="color:#000000"> _The dataset used is the Wine Dataset available at UCI\. This dataset has continuous features that are heterogeneous in scale due to differing properties that they measure \(i\.e alcohol content\, and malic acid\)\._ </span>

* <span style="color:#000000"> __Uses:__ </span>
  * <span style="color:#000000">Data Visualization</span>
  * <span style="color:#000000">Data Reduction</span>
  * <span style="color:#000000">Data Classification</span>
  * <span style="color:#000000">Noise Reduction</span>

* <span style="color:#000000"> __Examples__ </span>  <span style="color:#000000">:</span>
  * <span style="color:#000000">How many unique “sub\-sets” are in the sample?</span>
  * <span style="color:#000000">How are they similar / different?</span>
  * <span style="color:#000000">Which measurements are needed to differentiate?</span>
  * <span style="color:#000000">How to best present what is “interesting”?</span>
  * <span style="color:#000000">Which “sub\-set” does this new sample rightfully belong?</span>

* <span style="color:#000000">All examples will be available here: </span>  <span style="color:#000000">[https://github\.com/marcinwolter/MachineLearning2020](https://github.com/marcinwolter/MachineLearning2020)</span>
* <span style="color:#000000">[https://github\.com/marcinwolter/MachineLearning2020/blob/main/plot\_digits\_classif\.ipynb](https://github.com/marcinwolter/MachineLearning2020/blob/main/plot_digits_classif.ipynb)</span>  <span style="color:#000000">  \- iPython notebook prepared to run on Google Colaboratory </span>  <span style="color:#000000">[https://colab\.research\.google\.com/](https://colab.research.google.com/)</span>  <span style="color:#000000"> </span>
  * <span style="color:#000000">Reads handwritten digits</span>
  * <span style="color:#000000">Performs PCA</span>
  * <span style="color:#000000">Displays two first principal components:</span>
  * <span style="color:#000000">Classification using Naive Bayes and LDA</span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_219.png)

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_220.png)

<span style="color:#800000"> __Simple classificators__ </span>

* <span style="color:#000000">We have learned about:</span>
  * <span style="color:#000000">Disciminants:</span>
    * <span style="color:#000000">Cuts</span>
    * <span style="color:#000000">Fisher Linear Discriminant</span>
    * <span style="color:#000000">Naive Bayes</span>
  * <span style="color:#000000">Unsupervised method</span>
    * <span style="color:#000000">Decorrelation</span>
    * <span style="color:#000000">Principal Component Analysis</span>
* <span style="color:#000000">Today we will talk about:</span>
  * <span style="color:#000000">Overtraining problem\.</span>
  * <span style="color:#000000">Other discrimimants: k\-nearest neighbors\, decission tree</span>
  * <span style="color:#000000">Example how to use them and how to check their performance \(ROC curve\)\.</span>
  * <span style="color:#000000">Example of face classification – eigenfaces\.</span>

<span style="color:#800000"> __ROC curve__ </span>  <span style="color:#800000"> __\(see previous lecture\)__ </span>

* <span style="color:#000000">ROC  \(Receiver Operation Characteristic\) curve was first used to calibrate radars\.</span>
* <span style="color:#000000">Shows the background rejection \(1\-ε</span>  <span style="color:#000000">B</span>  <span style="color:#000000">\)</span>  <span style="color:#000000"> vs signal efficiency ε</span>  <span style="color:#000000">B</span>  <span style="color:#000000">\. Shows how good the classifier is\.</span>
* <span style="color:#000000">The integral of ROC could be a measure of the classifier quality:</span>
          * <span style="color:#800000">                                     </span>  <span style="color:#800000">Integral\(ROC\) = ½ – random</span>
          * <span style="color:#800000">                                     </span>  <span style="color:#800000">Integral\(ROC\) = 1  \- ideal</span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_221.png)

<span style="color:#800000"> __Classification Accuracy __ </span>

* <span style="color:#000000">Classification accuracy is the ratio of correct predictions to total predictions made\.</span>
  * <span style="color:#000000"> __classification accuracy = correct predictions / total predictions__ </span>
* <span style="color:#000000">It is often presented as a percentage by multiplying the result by 100\.</span>
  * <span style="color:#000000"> __classification accuracy = correct predictions / total predictions \* 100__ </span>
* <span style="color:#000000">Classification accuracy can also easily be turned into a misclassification rate or error rate by inverting the value\, such as:</span>
  * <span style="color:#000000"> __error rate = \(1 \- \(correct predictions / total predictions\)\) \* 100__ </span>

<span style="color:#800000"> __Confusion matrix__ </span>

<span style="color:#000000">Example confusion matrix \(recognition of dogs vs\. cats\)</span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_222.png)

<span style="color:#800000"> __Confusion Matrix__ </span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_223.png)

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_224.png)

<span style="color:#000000"> __Overtraining__ </span>  <span style="color:#000000"> – algorithm “learns” the particular events\, not the rules\.</span>

<span style="color:#800000"> _This effect important for all ML algorithms\._ </span>

<span style="color:#C9211E"> __Remedy – checking with another\, independent dataset\.__ </span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_225.png)

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_226.png)

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_227.png)

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_228.png)

<span style="color:#000000">Example of using Neural </span>

<span style="color:#000000">Network\.</span>

<span style="color:#800000"> __KNN \- k nearest neighbors__ </span>

<span style="color:#000000">Proposed already in 1951</span>

<span style="color:#000000">An event is qualified to the class\, to which belongs the majority of it’s k nearest neighbors\, </span>

<span style="color:#000000">or we calculate the probability of belonging to the class “signal” as:</span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_229.png)

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_230.png)

<span style="color:#800000"> _K_ </span>  <span style="color:#800000"> __\-Nearest Neighbors__ </span>

<span style="color:#270A29">The only parameter that can adjust the complexity of KNN is the number of neighbors </span>  <span style="color:#270A29"> _k_ </span>  <span style="color:#270A29">\. </span>

<span style="color:#270A29">The larger </span>  <span style="color:#270A29"> _k _ </span>  <span style="color:#270A29">is\, the smoother the classification boundary\. Or we can think of the complexity of KNN as lower when </span>  <span style="color:#270A29"> _k _ </span>  <span style="color:#270A29">increases\.</span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_231.png)

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_232.png)

<span style="color:#480048">KNN</span>

<span style="color:#480048">Decision Boundary</span>

<span style="color:#480048">Bayes Decision Boundary</span>

<span style="color:#800000"> _K_ </span>  <span style="color:#800000"> __\-Nearest Neighbors__ </span>

<span style="color:#270A29">For another simulated data set\, there are two classes\. The error rates based on the training data\, test data\, and </span>  <span style="color:#270A29"> _10_ </span>  <span style="color:#270A29">\-fold cross validation are plotted against </span>  <span style="color:#270A29"> _K_ </span>  <span style="color:#270A29">\, the number of neighbors\. </span>

<span style="color:#270A29">We can see that the training error rate tends to grow when </span>  <span style="color:#270A29"> _k _ </span>  <span style="color:#270A29">grows\, which is not the case for the error rate based on a separate test data set or cross\-validation\. </span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_233.png)

<span style="color:#000000">Decision tree – a series of cuts\, each „leaf” \(A\,B\,C\,D\,E\) has a label\, for example ”signal” and “background”\.</span>

<span style="color:#000000">Easy in visualization and interpretation</span>

<span style="color:#000000">Resistant for </span>  <span style="color:#000000"> _outliers_ </span>  <span style="color:#000000">\. </span>

<span style="color:#000000">Weak variables are ignored\.</span>

<span style="color:#000000">Fast training and classification\.</span>

<span style="color:#000000">Unfortunately: </span>  <span style="color:#000000"> __sensitive for fluctuations\, unstable__ </span>  <span style="color:#000000">\. </span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_234.png)

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_235.png)

<span style="color:#800000"> __Building the tree__ </span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_236.png)

<span style="color:#000000">We start from the root\.</span>

<span style="color:#000000">We divide the training sample by the best separating cut on the best variable\.</span>

<span style="color:#000000">We repeat the procedure until the stopping conditions are fulfilled\, for example: number of leafs\, number of events in a leaf etc\. </span>

<span style="color:#000000">The ratio S/B in a leaf defines the classification \(binary signal or background\, or a real number giving the probability\, that a given event is a signal\)\.</span>

<span style="color:#000000"> __Definitions of separation:__ </span>

<span style="color:#000000">Gini inpurity:  \(</span>  <span style="color:#000000"> _Corrado Gini 1912\, invented the Gini index used to measure the inequality of incomes_ </span>  <span style="color:#000000">\)</span>

<span style="color:#000000">p \(1\-p\)  : p= P\(signal\)\, </span>  <span style="color:#000000"> _purity_ </span>

<span style="color:#000000">Cross\-entropy:</span>

<span style="color:#000000">\-\(p ln p \+ \(1\-p\)ln\(1\-p\)\)</span>

<span style="color:#000000">Missidentification:</span>

<span style="color:#000000">1\-max\(p\,1\-p\)</span>

<span style="color:#800000"> __Example__ </span>  <span style="color:#800000"> __of simple classifiers__ </span>

<span style="color:#000000">[https://github\.com/marcinwolter/MachineLearning2020/blob/main/simple\_classifier\_comparison\.ipynb](https://github.com/marcinwolter/MachineLearning2020/blob/main/simple_classifier_comparison.ipynb)</span>  <span style="color:#000000"> </span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_237.png)

<span style="color:#800000"> __Classification of faces__ </span>

<span style="color:#000000">PCA – each face can be represented as a combination of a limited number of “eigenfaces”</span>

<span style="color:#000000">[https://github\.com/marcinwolter/MachineLearning2020/blob/main/plot\_face\_recognition\.ipynb](https://github.com/marcinwolter/MachineLearning2020/blob/main/plot_face_recognition.ipynb)</span>  <span style="color:#000000"> </span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_238.png)

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_239.png)

* <span style="color:#000000">We have learned about simple classifiers…\.</span>
* <span style="color:#000000">\.\.\.how to train them\, how to optimize them\, how to measure their performance\.\.\.</span>
* <span style="color:#000000">\.\.\.and about unsupervised methods like Principal Component Analysis\.</span>
* <span style="color:#000000">Next time we will talk about:</span>
  * <span style="color:#000000">Independent Component Analysis ICA</span>
  * <span style="color:#000000">Boosted Decision Trees BDT</span>

<span style="color:#800000"> __Backup – PCA in detail__ </span>

<span style="color:#000000"> __Calculate the empirical mean__ </span>

<span style="color:#000000">    </span>  <span style="color:#000000">Find the empirical mean along each column j = 1\, \.\.\.\, p\.</span>

<span style="color:#000000">    </span>  <span style="color:#000000">Place the calculated mean values into an empirical mean vector u of dimensions p × 1\.</span>

<span style="color:#000000"> __Calculate the deviations from the mean__ </span>

<span style="color:#000000"> __	__ </span>  <span style="color:#000000">Subtract the mean from each data point</span>

<span style="color:#000000"> __Find the covariance matrix__ </span>

<span style="color:#000000">    </span>  <span style="color:#000000">Find the p × p empirical covariance matrix </span>  <span style="color:#000000"> __C__ </span>  <span style="color:#000000"> from matrix </span>  <span style="color:#000000"> __B__ </span>  <span style="color:#000000"> \(data\):</span>

<span style="color:#000000"> __Find eigenvalues by solving:__ </span>

This means solving  <span style="color:#000000">a characteristic polynomial\.</span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_240.png)

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_241.png)

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_242.png)

<span style="color:#000000"> __Find the eigenvectors and eigenvalues of the covariance matrix__ </span>

<span style="color:#000000"> __   __ </span>  <span style="color:#000000"> </span>  <span style="color:#000000">Compute the matrix </span>  <span style="color:#000000"> __V__ </span>  <span style="color:#000000"> of eigenvectors which diagonalizes the covariance matrix </span>  <span style="color:#000000"> __C__ </span>  <span style="color:#000000">:</span>

<span style="color:#000000"> __Rearrange the eigenvectors and eigenvalues__ </span>

<span style="color:#000000">    </span>  <span style="color:#000000">Sort the columns of the eigenvector matrix V and eigenvalue matrix D in order of decreasing eigenvalue\.</span>

<span style="color:#000000"> __Select a subset of the eigenvectors as basis vectors__ </span>

<span style="color:#000000">     </span>  <span style="color:#000000">The goal is to choose a value of L as small as possible while achieving a reasonably high value of g on a percentage basis\. </span>

<span style="color:#000000"> __Project the z\-scores of the data onto the new basis__ </span>

<span style="color:#000000">You have reduced the dimensionality of your explaining as much variance as possible\.</span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_243.png)

__Find how much of variance is explained by n first principal components__

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_244.png)

<span style="color:#800000"> __Kernel density estimators__ </span>

<span style="color:#000000">Approximation of the unknown probability density as a sum of kernel functions centered in the points x</span>  <span style="color:#000000">n</span>  <span style="color:#000000"> of the training sample \(Parzen\, years 1960\-ties\)\. </span>

<span style="color:#000000">Typical kernel functions: Gauss\, 1/x</span>  <span style="color:#000000">n</span>  <span style="color:#000000"> itp\.</span>

<span style="color:#000000">Simple idea\, but using this method requires a lot of memory and CPU\.</span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_245.png)

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_246.png)

Approximated probability density \(blue\) comapred to the true probability density \(green\) as a function of the width of the Gaussian kernel function\. We can see\, that the width is a smoothing parameter\.

<span style="color:#800000"> __QUAERO package from the D__ </span>  <span style="color:#800000"> __0 experiment __ </span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_247.png)

_Phys\. Rev\. Lett\. 87 \(2001\) 231801\,_

<span style="color:#800000"> __PDE\_RS – extension of the Parzen methods__ </span>

<span style="color:#000000">\*published by T\. Carli\, B\. Koblitz\, NIM A 501 \(2003\) 576\-588</span>

<span style="color:#000000">Counts signal \(n</span>  <span style="color:#000000">s</span>  <span style="color:#000000">\) and background \(n</span>  <span style="color:#000000">b</span>  <span style="color:#000000">\) events in N\-dimensional cube around the classified event – only few events from the training sample are needed\. </span>

<span style="color:#000000">Size of the cube – a free parameter\.</span>

<span style="color:#000000">Discriminator</span>  <span style="color:#000000"> </span>  <span style="color:#000000"> _D\(x\)_ </span>  <span style="color:#000000"> :</span>

<span style="color:#000000">Simple analysis\.</span>

<span style="color:#000000">Events stored in the binary tree – the neighbor events are found quickly\.</span>

<span style="color:#000000">It’s a special case of Parzen method – the kernel function: </span>

![](https://raw.githubusercontent.com/marcinwolter/Machine-learning-KISD-2022/main/images/lecture2/MachineLearning2020_248.png)

